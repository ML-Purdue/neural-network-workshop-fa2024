{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0          5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4          9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "59995      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59997      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59998      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59999      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0          0      0      0      0      0      0      0      0  \n",
       "1          0      0      0      0      0      0      0      0  \n",
       "2          0      0      0      0      0      0      0      0  \n",
       "3          0      0      0      0      0      0      0      0  \n",
       "4          0      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "59995      0      0      0      0      0      0      0      0  \n",
       "59996      0      0      0      0      0      0      0      0  \n",
       "59997      0      0      0      0      0      0      0      0  \n",
       "59998      0      0      0      0      0      0      0      0  \n",
       "59999      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/mnist.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "print(28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that, we have 60000 images of 28x28 pixels in grayscale. We will use these images to train our model. Each image has 785 dimensions, 784 pixels for the image and 1 label for the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "# we shuffle the data\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 785)\n",
      "(12000, 785)\n"
     ]
    }
   ],
   "source": [
    "# In ML training, we split the data into training and testing data\n",
    "# We use the training data to train the model, and then we use the testing data to evaluate the model\n",
    "# the model CANNOT see the testing data during training\n",
    "# 80% of the data is used for training, and 20% is used for testing\n",
    "split_index = int(0.8 * data.shape[0])\n",
    "training_data = data[:split_index]\n",
    "testing_data = data[split_index:]\n",
    "print(training_data.shape)\n",
    "print(testing_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the training set into X and Y, where X is the image and Y is the label. We will use X to train our model and Y to validate our model. Since out data has the first column as the label, we will split the first column as Y and the rest as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000,)\n",
      "Example X data: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   5 127 156 231 255 254 254 102   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  16 140 217 253 253 253 254\n",
      " 253 253 247  87   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  96 235 254 253 253 240 174 175 211 253 253  88   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  26 166 244 253 244 206  87  16\n",
      "   0  10 175 253 253  65   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 118 253 237 155  83   0   0   0   0  97 253 253 253  96   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  97 254 254  57   0   0   0\n",
      "   0  10 171 255 254 254 117   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 141 253 192   5   0  20  28 147 220 253 254 243 106  18   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0 119 253 253 181 175\n",
      " 254 253 253 253 253 235 124   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  95 216 253 253 253 254 253 253 253 215  53   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5  58\n",
      "  58  96 254 253 253 200  80   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  10 134 255 254 213  15   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  31 174 253 254 191  12   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  37 146 247 253 213  42   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  25 213 253 253 222  53   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  76 231 253 253 185  14   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 108 231 255 254 183  45   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10 155 250 253 241 142  12   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  41 211 253 253 253  68   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  52 238 253 216  79   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  59 238 253  41   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "Example Y data: 9\n"
     ]
    }
   ],
   "source": [
    "training_data_X = training_data[:, 1:]\n",
    "training_data_Y = training_data[:, 0]\n",
    "testing_data_X = testing_data[:, 1:]\n",
    "testing_data_Y = testing_data[:, 0]\n",
    "\n",
    "print(training_data_X.shape)\n",
    "print(training_data_Y.shape)\n",
    "\n",
    "print(\"Example X data: \" + str(training_data_X[0]))\n",
    "print(\"Example Y data: \" + str(training_data_Y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the sigmoid function we discussed\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# defining the derivative of the sigmoid function, you can look up the proof for this\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "#converting the input Y labels to one hot encoding\n",
    "def convert_to_one_hot(Y):\n",
    "    num_data_points = Y.shape[0]\n",
    "    num_classes = np.max(Y) + 1 # we can also hardcode this to be 10\n",
    "    Y_one_hot = np.zeros((num_data_points, num_classes))\n",
    "    Y_one_hot[np.arange(num_data_points), Y] = 1\n",
    "    return Y_one_hot\n",
    "\n",
    "# loss function, we want this to become less and less as we train the model\n",
    "def loss_function(Y, Y_hat):\n",
    "    return 0.5 * np.sum((Y - Y_hat) ** 2) / Y.shape[0]\n",
    "\n",
    "def loss_function_derivative(Y, Y_hat):\n",
    "    return (Y_hat - Y)  / Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size # 784 for this dataset, since each image is 28x28 = 784\n",
    "        self.hidden_size = hidden_size # 16 from our discussion \n",
    "        self.output_size = output_size # 10 for this dataset, since we have 10 classes = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "\n",
    "        # we initialize the weights and biases randomly\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros(self.hidden_size)\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros(self.output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, Y, learning_rate):\n",
    "        # calculate the loss\n",
    "        loss = loss_function(Y, self.a2)\n",
    "        \n",
    "        dL_da2 = loss_function_derivative(Y, self.a2) \n",
    "        da2_dz2 = sigmoid_derivative(self.z2)\n",
    "        dL_dz2 = dL_da2 * da2_dz2\n",
    "        dz2_dW2 = self.a1\n",
    "\n",
    "        # calcuate the gradient of the loss with respect to w2 \n",
    "        dL_dW2 = np.dot(dz2_dW2.T, dL_dz2) \n",
    "        # calculate the gradient of the loss with respect to b2\n",
    "        dL_db2 = np.sum(dL_dz2, axis=0) \n",
    "        \n",
    "        \n",
    "        dz2_da1 = self.W2\n",
    "        da1_dz1 = sigmoid_derivative(self.z1)\n",
    "        dL_dz1 = np.dot(dL_dz2, dz2_da1.T) * da1_dz1\n",
    "        dz1_dW1 = X\n",
    "\n",
    "        # calculate the gradient of the loss with respect to w1\n",
    "        dL_dW1 = np.dot(dz1_dW1.T, dL_dz1)\n",
    "        # calculate the gradient of the loss with respect to b1\n",
    "        dL_db1 = np.sum(dL_dz1, axis=0)\n",
    "        \n",
    "        # update the weights and biases\n",
    "        self.W2 -= learning_rate * dL_dW2\n",
    "        self.b2 -= learning_rate * dL_db2\n",
    "        self.W1 -= learning_rate * dL_dW1\n",
    "        self.b1 -= learning_rate * dL_db1\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_batch(self, X, Y, batch_size = 32):\n",
    "        num_data_points = X.shape[0]\n",
    "        indices = np.random.choice(num_data_points, batch_size)\n",
    "        return X[indices], Y[indices]\n",
    "    \n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        num_batches = X.shape[0] // 128\n",
    "        for i in range(epochs):\n",
    "            for j in range(num_batches):\n",
    "                X_batch, Y_batch = self.get_batch(X, Y)\n",
    "                Y_hat = self.forward(X_batch)\n",
    "                loss = self.backward(X_batch, Y_batch, learning_rate)\n",
    "            print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def accuracy(self, X, Y):\n",
    "        Y_hat = self.predict(X)\n",
    "        Y_hat = np.argmax(Y_hat, axis=1)\n",
    "        return np.mean(Y_hat == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.5047825309009303\n",
      "Epoch: 1 Loss: 0.4500255617500031\n",
      "Epoch: 2 Loss: 0.41772520277569514\n",
      "Epoch: 3 Loss: 0.4183381118919983\n",
      "Epoch: 4 Loss: 0.417274581159526\n",
      "Epoch: 5 Loss: 0.3898347531558927\n",
      "Epoch: 6 Loss: 0.3849401420097928\n",
      "Epoch: 7 Loss: 0.390463566893657\n",
      "Epoch: 8 Loss: 0.3613280986484846\n",
      "Epoch: 9 Loss: 0.3458375217810087\n",
      "Epoch: 10 Loss: 0.30558190831709475\n",
      "Epoch: 11 Loss: 0.32267366546241755\n",
      "Epoch: 12 Loss: 0.364661121313295\n",
      "Epoch: 13 Loss: 0.35573778518768995\n",
      "Epoch: 14 Loss: 0.3047034787288481\n",
      "Epoch: 15 Loss: 0.28003117938774613\n",
      "Epoch: 16 Loss: 0.32963864661201575\n",
      "Epoch: 17 Loss: 0.3488788374652703\n",
      "Epoch: 18 Loss: 0.3154600166578092\n",
      "Epoch: 19 Loss: 0.32064892558524727\n",
      "Epoch: 20 Loss: 0.28420932261262766\n",
      "Epoch: 21 Loss: 0.32800925794675206\n",
      "Epoch: 22 Loss: 0.3267490879856827\n",
      "Epoch: 23 Loss: 0.24515729681113185\n",
      "Epoch: 24 Loss: 0.29469973553437523\n",
      "Epoch: 25 Loss: 0.3108416488692288\n",
      "Epoch: 26 Loss: 0.35454027263256266\n",
      "Epoch: 27 Loss: 0.2785577566618613\n",
      "Epoch: 28 Loss: 0.26191712060262645\n",
      "Epoch: 29 Loss: 0.2710969677969885\n",
      "Epoch: 30 Loss: 0.27848633325954303\n",
      "Epoch: 31 Loss: 0.21139037165858066\n",
      "Epoch: 32 Loss: 0.2705040010870966\n",
      "Epoch: 33 Loss: 0.23717741486156244\n",
      "Epoch: 34 Loss: 0.2787646496461621\n",
      "Epoch: 35 Loss: 0.28833426276029317\n",
      "Epoch: 36 Loss: 0.24123540517486228\n",
      "Epoch: 37 Loss: 0.2564679967683029\n",
      "Epoch: 38 Loss: 0.20402084493529948\n",
      "Epoch: 39 Loss: 0.2802037889169029\n",
      "Epoch: 40 Loss: 0.2265991687005729\n",
      "Epoch: 41 Loss: 0.2644968081167145\n",
      "Epoch: 42 Loss: 0.25563232728443164\n",
      "Epoch: 43 Loss: 0.17372054936962458\n",
      "Epoch: 44 Loss: 0.23340642639740938\n",
      "Epoch: 45 Loss: 0.2421730685969069\n",
      "Epoch: 46 Loss: 0.25673698905853937\n",
      "Epoch: 47 Loss: 0.23674562844344482\n",
      "Epoch: 48 Loss: 0.18733430191792838\n",
      "Epoch: 49 Loss: 0.22730204404669746\n",
      "Epoch: 50 Loss: 0.16974662159985526\n",
      "Epoch: 51 Loss: 0.231239498878547\n",
      "Epoch: 52 Loss: 0.15067873875577872\n",
      "Epoch: 53 Loss: 0.19471932868772535\n",
      "Epoch: 54 Loss: 0.184218434858132\n",
      "Epoch: 55 Loss: 0.19361057463589365\n",
      "Epoch: 56 Loss: 0.17295233365662072\n",
      "Epoch: 57 Loss: 0.20435147825354774\n",
      "Epoch: 58 Loss: 0.2220149409047737\n",
      "Epoch: 59 Loss: 0.2042015609282696\n",
      "Epoch: 60 Loss: 0.17373395880396234\n",
      "Epoch: 61 Loss: 0.25484262896329235\n",
      "Epoch: 62 Loss: 0.18935219373306073\n",
      "Epoch: 63 Loss: 0.20667902723932557\n",
      "Epoch: 64 Loss: 0.20595843454399831\n",
      "Epoch: 65 Loss: 0.2035972934190583\n",
      "Epoch: 66 Loss: 0.14653839890490833\n",
      "Epoch: 67 Loss: 0.09122926575831908\n",
      "Epoch: 68 Loss: 0.20217795938700966\n",
      "Epoch: 69 Loss: 0.19884393688272212\n",
      "Epoch: 70 Loss: 0.24720720136698113\n",
      "Epoch: 71 Loss: 0.20800098729970132\n",
      "Epoch: 72 Loss: 0.18439041577090048\n",
      "Epoch: 73 Loss: 0.2618697164639409\n",
      "Epoch: 74 Loss: 0.24816280644651018\n",
      "Epoch: 75 Loss: 0.12122815785733511\n",
      "Epoch: 76 Loss: 0.10864827590742557\n",
      "Epoch: 77 Loss: 0.1427612355591758\n",
      "Epoch: 78 Loss: 0.18732974780824163\n",
      "Epoch: 79 Loss: 0.1896519391336624\n",
      "Epoch: 80 Loss: 0.13567393520205645\n",
      "Epoch: 81 Loss: 0.24575006131723703\n",
      "Epoch: 82 Loss: 0.12335069577990809\n",
      "Epoch: 83 Loss: 0.22846819837519397\n",
      "Epoch: 84 Loss: 0.10570816535884779\n",
      "Epoch: 85 Loss: 0.20281358638684677\n",
      "Epoch: 86 Loss: 0.1800317769319943\n",
      "Epoch: 87 Loss: 0.16014746496616122\n",
      "Epoch: 88 Loss: 0.13318936985589347\n",
      "Epoch: 89 Loss: 0.1452489327319288\n",
      "Epoch: 90 Loss: 0.20959962276212457\n",
      "Epoch: 91 Loss: 0.21273220540879934\n",
      "Epoch: 92 Loss: 0.17955412728600204\n",
      "Epoch: 93 Loss: 0.18774786979412672\n",
      "Epoch: 94 Loss: 0.21552637655274262\n",
      "Epoch: 95 Loss: 0.10300493966675407\n",
      "Epoch: 96 Loss: 0.16625820073794362\n",
      "Epoch: 97 Loss: 0.19473814910018467\n",
      "Epoch: 98 Loss: 0.14408204416497733\n",
      "Epoch: 99 Loss: 0.18473914196501895\n",
      "0.7795\n"
     ]
    }
   ],
   "source": [
    "neuralnetwork = NeuralNetwork(784, 16, 10)\n",
    "\n",
    "training_data_Y_one_hot = convert_to_one_hot(training_data_Y)\n",
    "\n",
    "neuralnetwork.train(training_data_X, training_data_Y_one_hot, 100, 0.1)\n",
    "\n",
    "print(neuralnetwork.accuracy(testing_data_X, testing_data_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 7\n"
     ]
    }
   ],
   "source": [
    "def plot_image(X):\n",
    "    plt.imshow(X.reshape(28, 28), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "data_point = 69\n",
    "X = testing_data_X[data_point]\n",
    "plot_image(X)\n",
    "\n",
    "Y = testing_data_Y[data_point]\n",
    "Y_hat = neuralnetwork.predict(X)\n",
    "\n",
    "print(\"Predicted: \" + str(np.argmax(Y_hat)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
